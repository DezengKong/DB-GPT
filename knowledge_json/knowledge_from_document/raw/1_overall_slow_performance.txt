Unified Operations and Maintenance

Sharing fault pattern handling methods, post-incident reviews, technical insights, and empowerment courses with downstream and SRE (Site Reliability Engineering) teams.

[Operations Handbook] Analysis of Overall Slow Performance

[Problem Description]
Overall slow performance. Fails to meet customer job latency requirements or expectations.

[Problem Symptoms]
High latency reported for business interfaces; or elevated metrics such as database P80/P95; might involve a high number of slow SQL queries.

[Alerts]
Alerts related to business-side interface latency, success rates, etc. Database core P80/P95 related alerts.

[Business Impact]
Degraded business latency or inability to complete tasks within expected timeframes.

[Root Cause Analysis]
When addressing overall slow performance issues, before delving into system analysis data, it is advisable, if possible, to communicate with the customer first. Understand the context of the issue, such as customer expectations or goals, any business changes, types of operations, etc., to clarify the premises and goals of performance optimization.
For overall slow performance, identifying bottleneck points is crucial. Precise identification of bottleneck points directs performance optimization efforts, but eliminating one bottleneck might not guarantee a 100% improvement in performance, as the bottleneck might shift to another point. It's an iterative process that can be time-consuming.
There can be multiple reasons for overall slow performance, including but not limited to: 1. Business-side factors, 2. Insufficient system resources, 3. Suboptimal usage of database core resources, 4. Concurrency issues, 5. Suboptimal database configuration, 6. Suboptimal SQL queries, etc.

[Analysis Steps]

[Step One] Understand the background of overall slow performance, including customer expectations, business type, recent business changes, system modifications, etc.

[Step Two] Confirm whether the load has transferred to the kernel or if the bottleneck lies in the kernel. Check the CPU usage of the host where the database resides, relevant database kernel views, or OPS metrics. Determine whether the issue is on the business side or the database side. Investigate if any pressure has been transferred to the database kernel. Check for active sessions in the database and relevant OPS instance monitoring metrics.

[Step Three] If there is no evident pressure on the database side or if the pressure is not significant (e.g., low CPU usage, low active sessions), suggest investigating the business side. Common scenarios might include exhausted application server resources, high network latency, slow processing of query results leading to slow transaction execution, etc.

[Step Four] Investigate whether there are anomalies in system resources:
- High CPU usage: Check CPU usage via OPS metrics or operating system commands.
- High IO: Check IO statistics for high utilization or elevated read/write wait times.
- High memory usage: Identify abnormal memory consumption and potential xxxx process memory issues.
- Network anomalies: Address network connectivity issues, latency, and bandwidth saturation.

[Step Five] Investigate whether the database kernel is being optimally utilized:
- Concurrency issues: Address lock contention issues resulting from concurrent updates.
- Database configuration issues: Analyze GUC (Grand Unified Configuration) parameters and consider defaults.
- Exceptional wait events: Investigate and address abnormal wait events within the database.
- Long-term performance degradation: Examine WDR (Workload Distribution Report) reports for performance differences over time.
- Short-term performance degradation: Analyze WDR reports for short-term performance fluctuations.
- Suboptimal SQL: Identify and optimize suboptimal SQL queries affecting overall performance.

[Quick Recovery]
Recovery methods vary based on the specific problem. Sometimes coordination with the business side might be necessary for mitigation.